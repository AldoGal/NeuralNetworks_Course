{"cells":[{"metadata":{"_uuid":"ed4cd7c86887522b8c8d4b8c3c53ba1027949f9b"},"cell_type":"markdown","source":"# Description\nThe purpose of this kernel is to explore the new TensorFlow 2.0 API to achieve the ML results fast and easily with efficiency optimizations built-in.\n### Trying to automate the feature engineering and ensemble process. Both CPU and GPU compatible\n### This kernel is by no means comprehensive and accurate but serve as a starting point. \n### I hope it will make some people's life easier in getting started with new TensorFlow version.\nI will not be focussing on the utility provided by TensorFlow to convert old tf code to 2.0\n### Will try to see if we can get away without extensively preprocessing the data and get tensorflow to handle the data types"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Utilities\nimport datetime\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"../input\"))\n\n# Numericals\nimport numpy as np\nimport pandas as pd\n\n# Plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"darkgrid\")\n%matplotlib inline\n\n# TensorFlo 2.0\n!pip install -q tensorflow==2.0.0-alpha0\nimport tensorflow as tf\n# Load the TensorBoard notebook extension\n%load_ext tensorboard.notebook\n# Imports for the HParams plugin from tensorboard\nfrom tensorboard.plugins.hparams import api_pb2\nfrom tensorboard.plugins.hparams import summary as hparams_summary\nfrom google.protobuf import struct_pb2\n\n# Clear any logs from previous runs\n!rm -rf ./logs/ ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")\ntrain_data.info()\nprint('_'*50)\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23de2def20bde7868eb192a2c5edb9769bed11d9"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aa1d7bbd96a25ce685a1d8fe7fa9451e7b069d4"},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93aa50436d06d1fdece8970d469b0b329ef45d8c"},"cell_type":"markdown","source":"### Drop the columns if the missing values are more than 20% of the data. Drop missing rows otherwise"},{"metadata":{"trusted":true,"_uuid":"9b18e532419264e9ca433a984e3974a0065ba3a5"},"cell_type":"code","source":"for col in train_data.columns:\n    if len(train_data[col].dropna()) <= (0.7 * len(train_data)):\n        train_data.drop(columns=[col], inplace=True)\n    else:\n        train_data.dropna(axis=0, subset=[col],inplace=True)\n\nfor col in test_data.columns:\n    if len(test_data[col].dropna()) <= (0.7 * len(test_data)):\n        test_data.drop(columns=[col], inplace=True)\n    else:\n        test_data[col].fillna(value=test_data[col].mode()[0] ,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c09d970d277431fa78e24ec009ea51bdc64fa41"},"cell_type":"code","source":"train_data.info()\nprint('_'*50)\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6786728f751be8769471c80f5daadd20839ed7ff"},"cell_type":"markdown","source":"## No need to convert the data types from categorical to numerical because we want tensorflow to handle this kind of data"},{"metadata":{"_uuid":"af22d7039852eebe20955e0de24bc949093dbcfb"},"cell_type":"markdown","source":"## Define Feature columns for tensorflow\nExamples of each column type"},{"metadata":{"trusted":true,"_uuid":"f4994ae2c85b8c73ec22b312a12e15e369572280"},"cell_type":"code","source":"# Just to see the correlation\nplt.figure(figsize=(10,8))\nsns.heatmap(train_data.corr(method='pearson'),annot=True,cmap='YlGnBu',fmt='.2f',linewidths=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"652d3d496e465bf60d5aef33f79e6f180c082dc7"},"cell_type":"code","source":"feature_columns = []\n\n# numeric cols\nfor header in ['Age', 'Fare']:\n  feature_columns.append(tf.feature_column.numeric_column(header))\n\n# bucketized cols\nage = tf.feature_column.numeric_column(\"Age\")\nage_buckets = tf.feature_column.bucketized_column(age, boundaries=[5, 10, 20, 30, 40, 50, 60, 70, 80])\nfeature_columns.append(age_buckets)\n\n# indicator cols\ncategorical_cols = [\"Sex\", \"Embarked\", \"Pclass\", \"SibSp\", \"Parch\"]\nfor col in categorical_cols:\n    train_data[col] = train_data[col].apply(str)\n    test_data[col] = test_data[col].apply(str)\n    cat_column_with_vocab = tf.feature_column.categorical_column_with_vocabulary_list(\n          col, list(train_data[col].value_counts().index.values))\n    one_hot = tf.feature_column.indicator_column(cat_column_with_vocab)\n    feature_columns.append(one_hot)\n\n\n# embedding cols\nticket = tf.feature_column.categorical_column_with_hash_bucket(\"Ticket\", hash_bucket_size=1000)\nticket_embedding = tf.feature_column.embedding_column(ticket, dimension=8)\nfeature_columns.append(ticket_embedding)\n\n# crossed cols\np_class = tf.feature_column.categorical_column_with_vocabulary_list(\n          \"Pclass\", list(train_data[\"Pclass\"].value_counts().index.values))\nparch = tf.feature_column.categorical_column_with_vocabulary_list(\n          \"Parch\", list(train_data[\"Parch\"].value_counts().index.values))\npclass_parch_crossed = tf.feature_column.crossed_column([p_class, parch], hash_bucket_size=1000)\npclass_parch_crossed = tf.feature_column.indicator_column(pclass_parch_crossed)\nfeature_columns.append(pclass_parch_crossed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"869e25aea4077c3327535181afa45dd68e91a21e"},"cell_type":"code","source":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, testing=False, batch_size=32):\n    dataframe = dataframe.copy()\n    if not testing:\n        labels = dataframe.pop('Survived')\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81002699b6013bc68b4ec8b6f236d2ac372df3f0"},"cell_type":"code","source":"train_data, val_data = train_test_split(train_data, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1daff759cc6bd037835cda13c7ad1845257f77d"},"cell_type":"code","source":"batch_size = 32\ntrain_ds = df_to_dataset(train_data, batch_size=batch_size)\nval_ds = df_to_dataset(val_data, batch_size=batch_size)\ntest_ds = df_to_dataset(test_data, testing=True, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf4ed1dccc9630b9f75050016c4d5d4d450925c5"},"cell_type":"markdown","source":"# Setup Hyperparameter tuning"},{"metadata":{"trusted":true,"_uuid":"f72637b62bcdaaaab92fa467bbdedf3ea0f69f7c"},"cell_type":"code","source":"num_units_list = [128, 256]\ndropout_rate_list = [0.2, 0.5] \noptimizer_list = ['adam', 'sgd'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"365d776a86a163d36eba9eeedf318be60a46ad5d"},"cell_type":"code","source":"# Utility method to create summary for tensorboard\ndef create_experiment_summary(num_units_list, dropout_rate_list, optimizer_list):\n  num_units_list_val = struct_pb2.ListValue()\n  num_units_list_val.extend(num_units_list)\n  dropout_rate_list_val = struct_pb2.ListValue()\n  dropout_rate_list_val.extend(dropout_rate_list)\n  optimizer_list_val = struct_pb2.ListValue()\n  optimizer_list_val.extend(optimizer_list)\n  return hparams_summary.experiment_pb(\n      # The hyperparameters being changed\n      hparam_infos=[\n          api_pb2.HParamInfo(name='num_units',\n                             display_name='Number of units',\n                             type=api_pb2.DATA_TYPE_FLOAT64,\n                             domain_discrete=num_units_list_val),\n          api_pb2.HParamInfo(name='dropout_rate',\n                             display_name='Dropout rate',\n                             type=api_pb2.DATA_TYPE_FLOAT64,\n                             domain_discrete=dropout_rate_list_val),\n          api_pb2.HParamInfo(name='optimizer',\n                             display_name='Optimizer',\n                             type=api_pb2.DATA_TYPE_STRING,\n                             domain_discrete=optimizer_list_val)\n      ],\n      # The metrics being tracked\n      metric_infos=[\n          api_pb2.MetricInfo(\n              name=api_pb2.MetricName(\n                  tag='accuracy'),\n              display_name='Accuracy'),\n      ]\n  )\n\nexp_summary = create_experiment_summary(num_units_list, dropout_rate_list, optimizer_list)\nroot_logdir_writer = tf.summary.create_file_writer(\"logs/hparam_tuning\")\nwith root_logdir_writer.as_default():\n  tf.summary.import_event(tf.compat.v1.Event(summary=exp_summary).SerializeToString())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00c52b5e1da3208d7a718602f9783ba045f1742b"},"cell_type":"code","source":"# Model compiler\ndef train_test_model(hparams):\n\n  model = tf.keras.models.Sequential([\n    tf.keras.layers.DenseFeatures(feature_columns),\n    tf.keras.layers.Dense(hparams['num_units'], activation='relu'),\n    tf.keras.layers.Dropout(hparams['dropout_rate']),\n      tf.keras.layers.Dense(hparams['num_units'], activation='relu'),\n    tf.keras.layers.Dense(2, activation='sigmoid')\n  ])\n  model.compile(optimizer=hparams['optimizer'],\n                loss='binary_crossentropy',\n                metrics=['accuracy'])\n\n  model.fit(train_ds, \n            validation_data=val_ds, \n            epochs=50,\n            use_multiprocessing=True,\n            verbose=0)\n  _, accuracy = model.evaluate(val_ds)\n  return model, accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce40f4ef18a980e04f4647e25a97c39345fe3f64"},"cell_type":"code","source":"# Model runner\ndef run(run_dir, hparams):\n  writer = tf.summary.create_file_writer(run_dir)\n  summary_start = hparams_summary.session_start_pb(hparams=hparams)\n\n  with writer.as_default():\n    model, accuracy = train_test_model(hparams)\n    summary_end = hparams_summary.session_end_pb(api_pb2.STATUS_SUCCESS)\n      \n    tf.summary.scalar('accuracy', accuracy, step=1, description=\"The accuracy\")\n    tf.summary.import_event(tf.compat.v1.Event(summary=summary_start).SerializeToString())\n    tf.summary.import_event(tf.compat.v1.Event(summary=summary_end).SerializeToString())\n  return model, accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28941b640efbd4b5dd7973f072f2d4dc20ae2a8c"},"cell_type":"code","source":"model_dict = {}\nsession_num = 0\nfor num_units in num_units_list:\n    for dropout_rate in dropout_rate_list:\n        for optimizer in optimizer_list:\n            hparams = {'num_units': num_units, 'dropout_rate': dropout_rate, 'optimizer': optimizer}\n            print('--- Running training session %d' % (session_num + 1))\n            print(hparams)\n            run_name = \"run-%d\" % session_num\n            model, accuracy = run(\"logs/hparam_tuning/\" + run_name, hparams)\n            print(accuracy)\n            model_dict[accuracy] = model\n            session_num += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a3242fdeccecb9d49e00c2c0462bb81736165f"},"cell_type":"code","source":"best_model = model_dict[max(list(model_dict.keys()))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bc6e36e0a4064ebd24c1565e1388e700020bfe2"},"cell_type":"code","source":"predictions = best_model.predict(test_ds)\npredictions = np.argmax(predictions, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"861ce10c0b7becb306c1227f03ce236cf10fd780"},"cell_type":"code","source":"predictions_dataframe = test_data[[\"PassengerId\"]]\npredictions_dataframe[\"Survived\"] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0986b2e992c217af9c3d9c45a1ced4502e963afe"},"cell_type":"code","source":"predictions_dataframe.to_csv(\"gender_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f3149b86a86d81e8c7f1b71d5963f46b2632ea2"},"cell_type":"code","source":"best_model.save('best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47054dba0a3c78655f2a88c89e427e3d0c753eb9"},"cell_type":"markdown","source":"# It is this easy to save the model in tensorflow now. Reload is the same - with weights :)"},{"metadata":{"trusted":true,"_uuid":"758ae2aae4cbd9b855c001d28a6c2e8301812b52"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}