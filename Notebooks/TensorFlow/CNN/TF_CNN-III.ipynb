{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Neural Networks and Deep Learning</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Mixed-data neural network</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook applies a mixed data model architecture to a dataset of satellite images of areas where the most serious (serious and fatal) accidents occurred, in order to predict the locations of the most serious traffic accidents. [Source](https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "#import cv2\n",
    "import datetime\n",
    "import math\n",
    "import urllib\n",
    "import argparse\n",
    "import locale\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.ops import nearest_points\n",
    "import geopandas\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.display import SVG\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Number_of_Vehicles</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_of_Week</th>\n",
       "      <th>1st_Road_Class</th>\n",
       "      <th>Road_Type</th>\n",
       "      <th>Speed_limit</th>\n",
       "      <th>Junction_Detail</th>\n",
       "      <th>2nd_Road_Class</th>\n",
       "      <th>Light_Conditions</th>\n",
       "      <th>Weather_Conditions</th>\n",
       "      <th>Road_Surface_Conditions</th>\n",
       "      <th>Urban_or_Rural_Area</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Two_Hour_Groupings</th>\n",
       "      <th>Time_of_Day</th>\n",
       "      <th>Was_Daylight</th>\n",
       "      <th>Was_Bad_Weather</th>\n",
       "      <th>Was_Road_Dry</th>\n",
       "      <th>log_Number_of_Casualties</th>\n",
       "      <th>log_Number_of_Vehicles</th>\n",
       "      <th>LSOA</th>\n",
       "      <th>population_per_hectare</th>\n",
       "      <th>bicycle_aadf</th>\n",
       "      <th>motorbike_aadf</th>\n",
       "      <th>car_aadf</th>\n",
       "      <th>bus_aadf</th>\n",
       "      <th>light_goods_vehicle_aadf</th>\n",
       "      <th>heavy_goods_vehicle_aadf</th>\n",
       "      <th>Road</th>\n",
       "      <th>RCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201301BS70003</td>\n",
       "      <td>-0.171402</td>\n",
       "      <td>51.486361</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>A</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>30.0</td>\n",
       "      <td>T or staggered junction</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Urban</td>\n",
       "      <td>9</td>\n",
       "      <td>8am-10am</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>E01002844</td>\n",
       "      <td>110.8</td>\n",
       "      <td>1634.4</td>\n",
       "      <td>860.4</td>\n",
       "      <td>14888.0</td>\n",
       "      <td>1139.8</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>A3217</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201301BS70005</td>\n",
       "      <td>-0.173356</td>\n",
       "      <td>51.495115</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>Friday</td>\n",
       "      <td>A</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Crossroads</td>\n",
       "      <td>A</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>Other</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Urban</td>\n",
       "      <td>8</td>\n",
       "      <td>8am-10am</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>E01002821</td>\n",
       "      <td>74.6</td>\n",
       "      <td>559.6</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>28505.6</td>\n",
       "      <td>1396.2</td>\n",
       "      <td>3868.6</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>A4</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201301BS70006</td>\n",
       "      <td>-0.210767</td>\n",
       "      <td>51.518353</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Monday</td>\n",
       "      <td>B</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Crossroads</td>\n",
       "      <td>B</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Urban</td>\n",
       "      <td>11</td>\n",
       "      <td>10am-12pm</td>\n",
       "      <td>Office hours</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>E01002878</td>\n",
       "      <td>133.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3898.2</td>\n",
       "      <td>63274.8</td>\n",
       "      <td>763.4</td>\n",
       "      <td>15253.6</td>\n",
       "      <td>3185.8</td>\n",
       "      <td>A40</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201301BS70007</td>\n",
       "      <td>-0.209675</td>\n",
       "      <td>51.516808</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>B</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Crossroads</td>\n",
       "      <td>C</td>\n",
       "      <td>Daylight</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Urban</td>\n",
       "      <td>10</td>\n",
       "      <td>10am-12pm</td>\n",
       "      <td>Office hours</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>E01002831</td>\n",
       "      <td>179.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3898.2</td>\n",
       "      <td>63274.8</td>\n",
       "      <td>763.4</td>\n",
       "      <td>15253.6</td>\n",
       "      <td>3185.8</td>\n",
       "      <td>A40</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201301BS70009</td>\n",
       "      <td>-0.194332</td>\n",
       "      <td>51.492922</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>Friday</td>\n",
       "      <td>A</td>\n",
       "      <td>One way street</td>\n",
       "      <td>30.0</td>\n",
       "      <td>T or staggered junction</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>Darkness - lights lit</td>\n",
       "      <td>Fine no high winds</td>\n",
       "      <td>Dry</td>\n",
       "      <td>Urban</td>\n",
       "      <td>17</td>\n",
       "      <td>4pm-6pm</td>\n",
       "      <td>Rush hour</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>E01002851</td>\n",
       "      <td>272.3</td>\n",
       "      <td>869.2</td>\n",
       "      <td>1229.8</td>\n",
       "      <td>20478.6</td>\n",
       "      <td>897.2</td>\n",
       "      <td>4951.6</td>\n",
       "      <td>1251.4</td>\n",
       "      <td>A3220</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Index  Longitude   Latitude Accident_Severity  Number_of_Vehicles  \\\n",
       "0  201301BS70003  -0.171402  51.486361           Serious                   2   \n",
       "1  201301BS70005  -0.173356  51.495115            Slight                   1   \n",
       "2  201301BS70006  -0.210767  51.518353            Slight                   1   \n",
       "3  201301BS70007  -0.209675  51.516808            Slight                   2   \n",
       "4  201301BS70009  -0.194332  51.492922            Slight                   2   \n",
       "\n",
       "   Number_of_Casualties        Date Day_of_Week 1st_Road_Class  \\\n",
       "0                     1  2013-01-02   Wednesday              A   \n",
       "1                     2  2013-01-04      Friday              A   \n",
       "2                     1  2013-01-07      Monday              B   \n",
       "3                     1  2013-01-10    Thursday              B   \n",
       "4                     1  2013-01-04      Friday              A   \n",
       "\n",
       "            Road_Type  Speed_limit          Junction_Detail 2nd_Road_Class  \\\n",
       "0  Single carriageway         30.0  T or staggered junction   Unclassified   \n",
       "1  Single carriageway         30.0               Crossroads              A   \n",
       "2  Single carriageway         30.0               Crossroads              B   \n",
       "3  Single carriageway         30.0               Crossroads              C   \n",
       "4      One way street         30.0  T or staggered junction   Unclassified   \n",
       "\n",
       "        Light_Conditions  Weather_Conditions Road_Surface_Conditions  \\\n",
       "0               Daylight  Fine no high winds                     Dry   \n",
       "1               Daylight               Other                     Dry   \n",
       "2               Daylight  Fine no high winds                     Dry   \n",
       "3               Daylight  Fine no high winds                     Dry   \n",
       "4  Darkness - lights lit  Fine no high winds                     Dry   \n",
       "\n",
       "  Urban_or_Rural_Area  Hour Two_Hour_Groupings   Time_of_Day Was_Daylight  \\\n",
       "0               Urban     9           8am-10am       Morning          Yes   \n",
       "1               Urban     8           8am-10am       Morning          Yes   \n",
       "2               Urban    11          10am-12pm  Office hours          Yes   \n",
       "3               Urban    10          10am-12pm  Office hours          Yes   \n",
       "4               Urban    17            4pm-6pm     Rush hour           No   \n",
       "\n",
       "  Was_Bad_Weather Was_Road_Dry  log_Number_of_Casualties  \\\n",
       "0              No          Yes                  0.000000   \n",
       "1             Yes          Yes                  0.693147   \n",
       "2              No          Yes                  0.000000   \n",
       "3              No          Yes                  0.000000   \n",
       "4              No          Yes                  0.000000   \n",
       "\n",
       "   log_Number_of_Vehicles       LSOA  population_per_hectare  bicycle_aadf  \\\n",
       "0                0.693147  E01002844                   110.8        1634.4   \n",
       "1                0.000000  E01002821                    74.6         559.6   \n",
       "2                0.000000  E01002878                   133.4           2.6   \n",
       "3                0.693147  E01002831                   179.2           2.6   \n",
       "4                0.693147  E01002851                   272.3         869.2   \n",
       "\n",
       "   motorbike_aadf  car_aadf  bus_aadf  light_goods_vehicle_aadf  \\\n",
       "0           860.4   14888.0    1139.8                    2297.0   \n",
       "1          1516.0   28505.6    1396.2                    3868.6   \n",
       "2          3898.2   63274.8     763.4                   15253.6   \n",
       "3          3898.2   63274.8     763.4                   15253.6   \n",
       "4          1229.8   20478.6     897.2                    4951.6   \n",
       "\n",
       "   heavy_goods_vehicle_aadf   Road RCat  \n",
       "0                     352.0  A3217   PA  \n",
       "1                    1003.0     A4   PA  \n",
       "2                    3185.8    A40   PA  \n",
       "3                    3185.8    A40   PA  \n",
       "4                    1251.4  A3220   PA  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents = pd.read_csv('../../../Data/London_accidents_merged.csv')\n",
    "accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/geography/UK_LSOA_bounding_boxes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c1d93f70c470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This additional dataframe contains the latitude and longitude centre points for each LSOA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlsoa_latlong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/geography/UK_LSOA_bounding_boxes.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lsoa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Latitude'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Longitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlsoa_latlong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/envs/python_env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/geography/UK_LSOA_bounding_boxes.csv'"
     ]
    }
   ],
   "source": [
    "# This additional dataframe contains the latitude and longitude centre points for each LSOA\n",
    "lsoa_latlong = pd.read_csv('data/geography/UK_LSOA_bounding_boxes.csv', usecols=['lsoa', 'Latitude', 'Longitude'])\n",
    "lsoa_latlong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('data/population/Population_density.csv')\n",
    "population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = pd.read_csv('data/traffic/Traffic_averages.csv')\n",
    "traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining the motor vehicle traffic columns, based on the multi-collinearity found in model 1\n",
    "to_sum = ['motorbike_aadf', 'car_aadf', 'bus_aadf', 'light_goods_vehicle_aadf', 'heavy_goods_vehicle_aadf']\n",
    "traffic['motor_vehicle_aadf'] = traffic[to_sum].sum(axis=1)\n",
    "traffic.drop(to_sum, axis=1, inplace=True)\n",
    "traffic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset of 'danger' locations (with traffic accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-7XT79XZlNP"
   },
   "outputs": [],
   "source": [
    "def myround(x, base=.0005):\n",
    "    return base * round(x/base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rounded lat and long columns and a grid square column to the London accident dataset\n",
    "accidents['lat_4dp'] = myround(accidents['Latitude'])\n",
    "accidents['long_4dp'] = myround(accidents['Longitude'])\n",
    "accidents['grid_square'] = round(accidents['lat_4dp'],4).map(str) + \",\" + round(accidents['long_4dp'],4).map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just accidents that are serious/fatal and drop duplicates based on grid_square\n",
    "serious_fatal_accidents = accidents[accidents['Accident_Severity']!='Slight'].drop_duplicates(['grid_square'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serious_fatal_accidents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ac7280caad1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Take random sample of 10000 serious/fatal accidents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m serious_fatal_accidents_sample = serious_fatal_accidents.sample(n=10000, random_state=42, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                 replace=False).drop_duplicates(['grid_square'])\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserious_fatal_accidents_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mserious_fatal_accidents_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'serious_fatal_accidents' is not defined"
     ]
    }
   ],
   "source": [
    "# Take random sample of 10000 serious/fatal accidents\n",
    "serious_fatal_accidents_sample = serious_fatal_accidents.sample(n=10000, random_state=42, \n",
    "                                                                replace=False).drop_duplicates(['grid_square'])\n",
    "print(len(serious_fatal_accidents_sample))\n",
    "serious_fatal_accidents_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model 4 dataset just consisting of traffic and population features\n",
    "# Keep location features to get safe grid squares later\n",
    "danger_squares = serious_fatal_accidents_sample[['Latitude', 'Longitude', 'population_per_hectare', 'bicycle_aadf', \n",
    "                                'motorbike_aadf', 'car_aadf', 'bus_aadf', 'light_goods_vehicle_aadf', \n",
    "                                'heavy_goods_vehicle_aadf', 'lat_4dp', 'long_4dp', 'grid_square']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine traffic features for motor vehicles like we did in model 1 notebook\n",
    "to_sum = ['motorbike_aadf', 'car_aadf', 'bus_aadf', 'light_goods_vehicle_aadf', 'heavy_goods_vehicle_aadf']\n",
    "danger_squares['motor_vehicle_aadf'] = danger_squares[to_sum].sum(axis=1)\n",
    "danger_squares.drop(to_sum, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting columns in the right order\n",
    "danger_squares = danger_squares[['grid_square', 'Latitude', 'Longitude', 'population_per_hectare', \n",
    "                                 'bicycle_aadf', 'motor_vehicle_aadf']]\n",
    "\n",
    "# Rename columns for merging with safe dataset\n",
    "danger_squares = danger_squares.rename(columns={'Latitude': 'latitude', 'Longitude': 'longitude'})\n",
    "\n",
    "# Add safe column of all 0's due to being danger dataset\n",
    "danger_squares['safe'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of danger squares to download using Google Static Maps API\n",
    "model4_danger_squares = list(danger_squares.grid_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "danger_squares.set_index('grid_square', inplace=True)\n",
    "danger_squares.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset for model 4 notebook\n",
    "#danger_squares.to_csv('model4_danger_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset of 'safe' locations (with no accidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same dataset of safe squares will be used from model 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the same safe squares dataset (with no accidents) as was used in model 3\n",
    "safe_squares = pd.read_csv('data/modelling/model3_safe_dataset.csv')\n",
    "safe_squares.set_index('grid_square', inplace=True)\n",
    "safe_squares.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets of safe and danger squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the final dataset containing safe and danger squares\n",
    "df = safe_squares.append(danger_squares)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/modelling/model4_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modelling/model4_dataset.csv', index_col='grid_square')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example images from the serious/fatal class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images below show the first five satellite images of areas where serious or fatal accidents occurred. They are primarily large motorways (highways) with multiple lanes, on straight sections of road. This is expected based on the results of the EDA of serious and fatal accidents in notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_path, size=[600, 400]):\n",
    "    img = load_img(image_path, target_size=(size[0], size[1]))\n",
    "    img_tensor = img_to_array(img)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "    img_tensor /= 255.\n",
    "    plt.imshow(img_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_danger_path = 'model4_images/fatal_or_serious/'\n",
    "imgs_danger = [file for file in os.listdir(imgs_danger_path) if file.endswith('.jpg')]\n",
    "\n",
    "imgs_danger_plot = []\n",
    "for img in imgs_danger[:5]:\n",
    "    imgs_danger_plot.append(os.path.join(imgs_danger_path, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,7))\n",
    "for i, img in enumerate(imgs_danger_plot):\n",
    "    fig.add_subplot(1,5,i+1)\n",
    "    plt.suptitle('Example areas where serious accidents occurred', fontsize=15)\n",
    "    show_image(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model iteration 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists of grid squares from the safe and serious square sets were used to download images from the Google Static Maps API using the same code described in notebook 2, into a folder called 'model4_images'.\n",
    "\n",
    "The same model structure will be used as the best model produced for model 3 (version 3), in order to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting images and reshaping\n",
    "image_folder = 'model4_images/'\n",
    "image_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        image_folder, shuffle=False, class_mode='binary',\n",
    "        target_size=(128, 128), batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the labels\n",
    "image_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ordered list of filenames for the images\n",
    "image_files = pd.Series(image_generator.filenames)\n",
    "image_files = image_files.str.split('/', expand=True)[1].str[:-4]\n",
    "image_files = list(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sorting the structured data into the same order as the images\n",
    "df_sorted = df.reindex(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double checking the dataframe is in the same order as the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is for the correct grid square, confirming that the dataframe is in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will be used to pre-process the data and create the mixed-input neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_structured_data(df, train, test):\n",
    "    \"\"\"\n",
    "    Pre-processes the given dataframe by minmaxscaling the continuous features (fit-transforming the training data and transforming the test data)\n",
    "    \"\"\"\n",
    "    continuous = [\"population_per_hectare\", \"bicycle_aadf\", \"motor_vehicle_aadf\"]\n",
    "    cs = MinMaxScaler()\n",
    "    trainX = cs.fit_transform(train[continuous])\n",
    "    testX = cs.transform(test[continuous])\n",
    "    return (trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regularizer=None):\n",
    "    \"\"\"Creates a simple two-layer MLP with inputs of the given dimension\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    model.add(Dense(4, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regularizer=None):\n",
    "    \"\"\"\n",
    "    Creates a CNN with the given input dimension and filter numbers.\n",
    "    Adapted from the function described here: https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
    "    \"\"\"\n",
    "    # Initialize the input shape and channel dimension, where the number of channels is the last dimension\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    " \n",
    "    # Define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    " \n",
    "    # Loop over the number of filters \n",
    "    for (i, f) in enumerate(filters):\n",
    "        # If this is the first CONV layer then set the input appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    " \n",
    "        # Create loops of CONV => RELU => BN => POOL layers\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        \n",
    "    # Final layers - flatten the volume, then Fully-Connected => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16, kernel_regularizer=regularizer)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    " \n",
    "    # Apply another fully-connected layer, this one to match the number of nodes coming out of the MLP\n",
    "    x = Dense(4, kernel_regularizer=regularizer)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    " \n",
    "    # Construct the CNN\n",
    "    model = Model(inputs, x)\n",
    " \n",
    "    # Return the CNN\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will be used for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cf(y_true, y_pred, class_names=None, model_name=None):\n",
    "    \"\"\"Plots a confusion matrix\"\"\"\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    plt.imshow(cf, cmap=plt.cm.Blues)\n",
    "    \n",
    "    if model_name:\n",
    "        plt.title(\"Confusion Matrix: {}\".format(model_name))\n",
    "    else:\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    class_names = set(y_true)\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    if class_names:\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    thresh = cf.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cf.shape[0]), range(cf.shape[1])):\n",
    "        plt.text(j, i, cf[i, j], horizontalalignment='center', color='white' if cf[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_evaluation(model, history, train_features, train_images, train_labels, test_features, test_images, test_labels, class_names=None, model_name=None):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a CNN with loss and accuracy plots, a confusion matrix and a classification report for the training and test sets.\n",
    "    \"\"\"\n",
    "    train_acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epch = range(1, len(train_acc) + 1)\n",
    "    plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "    plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "    plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    results_test = model.evaluate([test_features, test_images], test_labels)\n",
    "    print('Test Loss:', results_test[0])\n",
    "    print('Test Accuracy:', results_test[1])\n",
    "    \n",
    "    y_train_pred = np.round(model.predict([train_features, train_images]))\n",
    "    y_pred = np.round(model.predict([test_features, test_images]))\n",
    "    \n",
    "    show_cf(test_labels, y_pred, class_names=class_names, model_name=model_name)\n",
    "    \n",
    "    print(classification_report(train_labels, y_train_pred))\n",
    "    print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conducting the train test split:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_test_split to partition the training and testing structured data attributes and images\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = train_test_split(df_sorted, images, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the labels for y as the safe column\n",
    "trainY = trainAttrX[\"safe\"]\n",
    "testY = testAttrX[\"safe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the structured data\n",
    "(trainAttrX, testAttrX) = process_structured_data(df_sorted, trainAttrX, testAttrX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP and CNN models\n",
    "mlp1 = create_mlp(trainAttrX.shape[1], regularizer=regularizers.l1(0.005))\n",
    "cnn1 = create_cnn(128, 128, 3, regularizer=regularizers.l1(0.005))\n",
    " \n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = concatenate([mlp1.output, cnn1.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mlp1.summary())\n",
    "print(cnn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final FC layer head will have two dense layers\n",
    "x = Dense(4, activation=\"relu\", kernel_regularizer=regularizers.l1(0.005))(combinedInput)\n",
    "x = Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l1(0.005))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "model1 = Model(inputs=[mlp1.input, cnn1.input], outputs=x)\n",
    "\n",
    "# compile the model \n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model1.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=opt)\n",
    " \n",
    "# train the model, and validate with the first 1000 rows of the test set\n",
    "model1_history = model1.fit([trainAttrX, trainImagesX], trainY, validation_data=([testAttrX[:1000], testImagesX[:1000]], testY[:1000]), epochs=15, batch_size=10)\n",
    " \n",
    "end = datetime.datetime.now()\n",
    "print(\"Time taken to run:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('models/mixed_model4_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_evaluation(model1, model1_history, trainAttrX, trainImagesX, trainY, testAttrX[1000:], testImagesX[1000:], testY[1000:], class_names=['serious/fatal', 'safe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is performing well, with an average F1 score of 0.82 (better than the best version of model 3, which scored 0.80). There is no overfitting, and there is a particularly low number of false negatives, which is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section uses the best model (iteration 1) to produce a dataframe of predictions for each location in the test set. It also shows some examples of correct model predictions for areas with no accidents and areas with serious/fatal accidents in both rural and urban areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model1 = load_model('models/mixed_model4_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the model to make predictions for each of the locations (with images and structured data attributes) in the test set\n",
    "y_pred = np.round(model1.predict([testAttrX[1000:], testImagesX[1000:]]))\n",
    "\n",
    "# Reshaping the predictions and turning them into a pandas series\n",
    "test_predictions = pd.Series(y_pred.reshape((4000,)))\n",
    "test_predictions = pd.Series(test_predictions)\n",
    "\n",
    "# Getting a dataframe of the test set locations and actual classes\n",
    "test_actuals = pd.DataFrame(testY[1000:]).reset_index()\n",
    "\n",
    "# Making a dataframe of True and Predicted labels so can look up images by grid_square\n",
    "test_df = pd.concat([test_actuals, test_predictions], axis=1)\n",
    "test_df.columns = ['grid_square', 'True', 'Predicted']\n",
    "\n",
    "# Example predictions\n",
    "test_df[40:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example images:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f99cd47450c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Used grid_square column to check urban or rural areas in Google Maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Plot examples of safe and fatal/serious images from urban and rural areas to add to presentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestImagesX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Used grid_square column to check urban or rural areas in Google Maps\n",
    "# Plot examples of safe and fatal/serious images from urban and rural areas to add to presentation\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(testImagesX[1000,:,:], cmap='gray')\n",
    "plt.title('safe')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(testImagesX[1008,:,:], cmap='gray')\n",
    "plt.title('fatal/serious')\n",
    "plt.axis('off')\n",
    "plt.suptitle('Rural')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(testImagesX[1029,:,:], cmap='gray')\n",
    "plt.title('safe')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(testImagesX[1041,:,:], cmap='gray')\n",
    "plt.title('fatal/serious')\n",
    "plt.axis('off')\n",
    "plt.suptitle('Urban')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model iteration 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one final version, this iteration of the model will add an extra layer to the MLP branch of the neural network, as it is currently only one small layer and an output layer. It will also be run for slightly more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting images and reshaping\n",
    "image_folder = 'model4_images/'\n",
    "image_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        image_folder, shuffle=False, class_mode='binary',\n",
    "        target_size=(128, 128), batch_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the labels\n",
    "image_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the ordered list of filenames for the images\n",
    "image_files = pd.Series(image_generator.filenames)\n",
    "image_files = image_files.str.split(\"\\\\\", expand=True)[1].str[:-4]\n",
    "image_files = list(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sorting the structured data into the same order as the images\n",
    "df_sorted = df.reindex(image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an extra layer to the 'create MLP' function\n",
    "def create_mlp(dim, regularizer=None):\n",
    "    \"\"\"Creates a three-layer MLP with inputs of the given dimension\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=dim, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    model.add(Dense(32, input_dim=dim, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    model.add(Dense(4, activation=\"relu\", kernel_regularizer=regularizer))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MLP and CNN models\n",
    "mlp2 = create_mlp(trainAttrX.shape[1], regularizer=regularizers.l1(0.005))\n",
    "cnn2 = create_cnn(128, 128, 3, regularizer=regularizers.l1(0.005))\n",
    " \n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = concatenate([mlp2.output, cnn2.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mlp2.summary())\n",
    "print(cnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final FC layer head will have two dense layers\n",
    "x = Dense(4, activation=\"relu\", kernel_regularizer=regularizers.l1(0.005))(combinedInput)\n",
    "x = Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l1(0.005))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(inputs=[mlp2.input, cnn2.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the model\n",
    "SVG(model_to_dot(model2, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model2, to_file='mixed_model4_v2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "\n",
    "# compile the model \n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model2.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=opt)\n",
    " \n",
    "# train the model, and validate with the first 1000 rows of the test set\n",
    "model2_history = model2.fit([trainAttrX, trainImagesX], trainY, validation_data=([testAttrX[:1000], testImagesX[:1000]], testY[:1000]), epochs=20, batch_size=10)\n",
    " \n",
    "end = datetime.datetime.now()\n",
    "print(\"Time taken to run:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('models/mixed_model4_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_evaluation(model2, model2_history, trainAttrX, trainImagesX, trainY, testAttrX[1000:], testImagesX[1000:], testY[1000:], class_names=['serious', 'safe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs slightly worse than the first version, on the basis of both the F1 score and the recall rate/proportion of false negatives. Therefore, the preferred model is version 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and potential directions for future work\n",
    "\n",
    "The best model produced was version 1, which had an average F1 score of 0.82 and had no overfitting.\n",
    "\n",
    "To try and improve the accuracy of the model, some of the following could be tried in further model iterations:\n",
    "- Increasing the image size for input into the CNN branch\n",
    "- Using a pre-trained model as the base for the CNN (which increased accuracy in model 2)\n",
    "- Parameter tuning of both the CNN and MLP components (e.g. experimenting with regularization and optimization)\n",
    "- Parameter tuning of the final fully-connected layer head\n",
    "- Adding additional data to the structured dataset about the local area\n",
    "- Limiting the 'safe square' part of the dataset to only squares that contain roads of some type, i.e. excluding fields, forests and bodies of water"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
