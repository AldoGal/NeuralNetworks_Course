{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Neural Networks and Deep Learning</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">PyTorch and TensorFlow Comparison</h2>\n",
    "\n",
    "[Source](https://towardsdatascience.com/pytorch-vs-tensorflow-in-code-ada936fd5406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both TensorFlow and PyTorch are machine learning frameworks specifically designed for developing deep learning algorithms with access to the computational power needed to process lots of data (e.g. parallel computing, training on GPUs, etc).\n",
    "\n",
    "TensorFlow, which comes out of Google, was released in 2015 under the Apache 2.0 license. In Oktober 2019, TensorFlow 2.0 was released, which is said to be a huge improvement. It’s typically used in Python. PyTorch, on the other hand, comes out of Facebook and was released in 2016 under a similarly permissive open source license. As its name suggests, it’s also a Python library.\n",
    "Model Definition\n",
    "\n",
    "Back to the main reason for this blog post. The plan is to implement a simple neural network architecture in both TensorFlow and PyTorch to see some of the similarities and differences.\n",
    "\n",
    "The neural network model consists of three layers: Embedding Layer → Global Average Pooling Layer → Dense Layer.\n",
    "\n",
    "#### Dataset and Preprocessing\n",
    "\n",
    "The dataset used here consists of 1600,000 tweets and their sentiment (0=negative, 1=positive). First, loading the data from a CSV file and displaying some rows of the data frame to get an idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.kaggle.com/kazanova/sentiment140\n",
    "#df = pd.read_csv('./training.1600000.processed.noemoticon.csv', engine='python', usecols=[0,5], encoding='utf-8', names=['sentiments', 'tweets'])\n",
    "#df.sentiments = df.sentiments.map({0:0, 4:1})\n",
    "#df.to_csv('../Data/tweets.csv.gz', encoding='utf-8', compression=\"gzip\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiments                                             tweets\n",
       "0           0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1           0  is upset that he can't update his Facebook by ...\n",
       "2           0  @Kenichan I dived many times for the ball. Man...\n",
       "3           0    my whole body feels itchy and like its on fire \n",
       "4           0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"../Data/tweets.csv.gz\"\n",
    "data = pd.read_csv(data_path, encoding='utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   sentiments  1600000 non-null  int64 \n",
      " 1   tweets      1600000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Preprocessing\n",
    "\n",
    "To vectorize the tweets, I used Keras’ tokenizer here but there’re countless others that can do the same or even more.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token='<00v>')\n",
    "tokenizer.fit_on_texts(data.tweets)\n",
    "\n",
    "# transform tweets into sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(data.tweets)\n",
    "\n",
    "# pad sequences so that they have uniform lenth\n",
    "padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=42)\n",
    "assert(padded.shape==(1600000,42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = padded\n",
    "labels = np.array(data.sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what happens in the code snippet above. We assign an integer to each of the 20,000 most common words of the tweets and then turn the tweets into sequences of integers. We pad shorter ones with zeros and cut off longer ones, forcing a sequence length of 42. Finally, we should have a matrix of dimension 40,000 x 42 (tweets x sequence length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TensorFlow models is typically done using Keras. Keras is built on top of TensorFlow and allows for easy and fast prototyping because it has many layers built-in — it would be tedious or even prohibitive to code them from scratch each time.\n",
    "\n",
    "There are three ways to build a neural network model in Keras.\n",
    "\n",
    "## 1. Subclassing API\n",
    "\n",
    "You can create your own fully-customizable models by subclassing the tf.keras.Model class and implementing the forward pass in the call method. Put differently, layers are defined in the __init__() method and the logic of the forward pass in the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subclass_Model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, embedding_dim=25):\n",
    "        \n",
    "        super(Subclass_Model, self).__init__()\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_dim=20000,\n",
    "                                                         output_dim=50,\n",
    "                                                         input_length=42)\n",
    "        \n",
    "        self.pool1D_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.fc_layer =  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.pool1D_layer(x)\n",
    "        return self.fc_layer(x)\n",
    "    \n",
    "model = Subclass_Model()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of using such an object-oriented approach is that you can reuse layers multiple times within the call method or define a more complex forward pass. But nothing like that is happening in this example here, it’s just a linear stack of layers.\n",
    "\n",
    "## 2. Functional API\n",
    "\n",
    "In the functional API, given some input tensor(s) and output tensor(s), you can also instantiate a Model. It’s a user-friendly way to build a neural network and Keras even recommends it over model subclassing. With this approach, you essentially define a layer and immediately pass it the input of the previous layer. So it requires slightly less coding with the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 42)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 42, 50)            1000000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,000,051\n",
      "Trainable params: 1,000,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(42,))\n",
    "x = tf.keras.layers.Embedding(input_dim=20000,\n",
    "                              output_dim=50,\n",
    "                              input_length=42)(inputs)\n",
    "\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sequential API\n",
    "\n",
    "The sequential API is the most compact way to define a model and sufficient for certain (simple) neural networks, typically consisting of just a few common layers — kind of a shortcut to a trainable model. It’s definitely convenient and works well but is too inflexible if you wish to implement more sophisticated ideas.\n",
    "\n",
    "Training a Neural Network in Keras\n",
    "\n",
    "Before you can train a Keras model, it must be compiled by running the model.compile() function, which is also where you specify the loss function and optimizer.\n",
    "\n",
    "Regardless of how you build a Keras model, there are two functions I’d like to show you. First, calling model.summary() prints a compact summary of the model and the number of parameters, which is super useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 42, 50)            1000000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,000,051\n",
      "Trainable params: 1,000,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim=20000,\n",
    "                                    output_dim=50,\n",
    "                                    input_length=42))\n",
    "\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Keras model\n",
    "\n",
    "Keras models have a convenient fit function for training a model (just like Scikit-Learn), which also takes care of batch processing and even evaluates the model on the run (if you tell it to do so).  \n",
    "Note: It’s okay to pass Numpy arrays as inputs to the fit function even though TensorFlow (PyTorch too for that matter) operates on tensors only, which is a similar data structure but optimized for matrix computations. Keras takes care of transforming the arrays under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(x=seq,\n",
    "          y=labels,\n",
    "          batch_size=32,\n",
    "          epochs=3,\n",
    "          verbose=2,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, by calling tf.keras.utils.plot_model() you get a graphical summary of the model. \n",
    "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In oder to process the data in batches, a dataloader must be created. The dataloader returns one batch at a time in a dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tweets, sentiments):\n",
    "        self.tweets = tweets\n",
    "        self.sentiments = sentiments\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sample = {\"tweets\":torch.LongTensor(self.tweets[index,:]),\n",
    "                  \"sentiments\":self.sentiments[index]}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.tweets.shape[0]\n",
    "    \n",
    "    \n",
    "tweets_dataset = MyDataset(seq, labels.astype(float))\n",
    "dataloader = DataLoader(tweets_dataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to build a neural network model in PyTorch.\n",
    "\n",
    "## 1. Subclassing\n",
    "\n",
    "Similar to TensorFlow, in PyTorch you subclass the nn.Model module and define your layers in the __init__() method.  \n",
    "The only difference is that you create the forward pass in a method named forward instead of call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=20000,\n",
    "                                            embedding_dim=50)\n",
    "        self.pooling_layer = nn.AvgPool1d(kernel_size=50)\n",
    "        self.fc_layer = nn.Linear(in_features=42, out_features=1)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.pooling_layer(x).view(32, 42)\n",
    "        \n",
    "        return torch.sigmoid(self.fc_layer(x))\n",
    "    \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Difference to the Keras model: There’s only an average-pooling layer in PyTorch so it needs to have the right kernel size in order the make it global average-pooling.\n",
    "\n",
    "## 2. Sequential\n",
    "\n",
    "PyTorch also offers a Sequential module that looks almost equivalent to TensorFlow’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=20000, embedding_dim=50),\n",
    "    nn.AvgPool1d(kernel_size=50),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(in_features=42, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I found that many layers do not work with PyTorch’s nn.Sequential such as many recurrent layers (RNNs, LSTMS, etc.). In fact, PyTorch didn’t really want to implement the sequential module at all because it wants developers to use subclassing.\n",
    "\n",
    "## Training a PyTorch Model\n",
    "\n",
    "There’s no pre-made fit function for PyTorch models, so the training loop needs to be implemented from scratch. Here’s what a typical training loop in PyTorch looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch training loop\n",
    "\n",
    "#define the loss fn and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# empty list to track batch losses\n",
    "batch_losses = []\n",
    "\n",
    "#train the neural network for 5 epochs\n",
    "for epoch in range(1):\n",
    "\n",
    "    #reset iterator\n",
    "    dataiter = iter(dataloader)\n",
    "    \n",
    "    #iterate over dataset\n",
    "    for batch in dataiter:\n",
    "                \n",
    "        #reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward propagation through the network\n",
    "        out = model(batch[\"tweets\"])\n",
    "        \n",
    "        #print(out.shape, batch[\"sentiments\"].shape)\n",
    "        #calculate the loss\n",
    "        loss = criterion(out, batch[\"sentiments\"])\n",
    "        \n",
    "        #track batch loss\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        #update the parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short description of the training loop: For each batch, we calculate the loss and then call loss.backward() to backpropagate the gradient through the layers. In addition, we call optimizer.step() to tell the optimizer to update the parameters. For a more detailed description of how to train a PyTorch model see [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
