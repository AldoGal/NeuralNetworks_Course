{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "N = 64\n",
    "D_in = 1000\n",
    "H = 100\n",
    "D_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce random input data, but we dont need gradients on them\n",
    "x = Variable(torch.randn(N, D_in), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1561,  0.4000, -0.8615, -0.4835, -0.3076,  0.3016, -0.9939,  0.4709,\n",
       "         0.4287,  0.5394])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data, as a tensor are accessible via .data (here retrieving only \n",
    "# first ten entries of second column)\n",
    "x.data[:10:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1561, 0.4000, 0.0000, 0.0000, 0.0000, 0.3016, 0.0000, 0.4709, 0.4287,\n",
       "        0.5394])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## variables have all the convenience methods of tensors\n",
    "## https://pytorch.org/docs/stable/generated/torch.clamp.html\n",
    "x.clamp(min=0.)[:10:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce random labels, and no gradients needed on them, either\n",
    "y = Variable(torch.randn(N, D_out), requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights however, should remember their gradients\n",
    "w1 = Variable (torch.randn (D_in, H), requires_grad = True)\n",
    "w2 = Variable (torch.randn (H, D_out), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# the gradient is also a Variable, and accessible via .grad\n",
    "#(but only after backprop)\n",
    "print (w1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=mm#torch.mm\n",
    "y_pred = x.mm(w1).clamp(min=0.).mm(w2)\n",
    "loss = (y_pred - y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all variables track their provenance\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12386.5020,  -2801.8186,  -1365.5881,  10750.7256,  -3033.4141,\n",
       "        -10002.8525,  15974.6494,  -7102.8984,  -2417.5420,   5025.6353])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have a gradient\n",
    "w1.grad[:10:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this class implements a ReLU function, the backward pass implements\n",
    "# the partial derivative (1 for x > 0., 0 for x < 0.). \n",
    "class ReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        self.save_for_backward(x)\n",
    "        return x.clamp(min=0.)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_y):\n",
    "        x, = self.saved_tensors\n",
    "        grad_input = grad_y.clone()\n",
    "        grad_input[x<0.]=0.\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"calling\" the object with 'apply' invokes the .forward member function\n",
    "f.apply(torch.autograd.Variable(torch.Tensor([3.,-3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random input tensor\n",
    "x = torch.randn(N, D_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4367, -1.5982, -0.2146,  ...,  0.3118, -0.2496, -0.7362],\n",
       "        [-1.4542, -0.5148,  0.9165,  ...,  0.0098,  1.6181, -0.9094],\n",
       "        [-0.2675, -0.2521,  0.1953,  ...,  0.7910,  1.2410, -0.0043],\n",
       "        ...,\n",
       "        [-1.2094,  1.1401,  0.7764,  ...,  1.4210, -0.9036,  1.1800],\n",
       "        [-1.4131,  0.2246, -0.4104,  ..., -0.9007, -1.2825,  0.5247],\n",
       "        [-0.7374, -0.0160, -0.3361,  ...,  1.4680,  1.3157, -0.8704]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print it\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the shape of your tensor\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the type of your tensor\n",
    "x.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on what device does your tensor live?\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the object to the GPU / CPU\n",
    "x = x.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4367, -1.5982, -0.2146,  ...,  1.4680,  1.3157, -0.8704])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reinterpret the second order tensor as one large first order tensor\n",
    "x.view(64000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random input tensor\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform forward step in a neural network manually\n",
    "h = x.mm(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a \"RELU\" activation function manually\n",
    "h_relu = h.clamp(min=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second linear layer\n",
    "y_pred = h_relu.mm(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the \"loss\" of the network\n",
    "loss = (y_pred - y).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24643262.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually compute the gradient of the loss\n",
    "grad_y_pred = 2 * (y_pred - y)\n",
    "grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "grad_h = grad_h_relu.clone()\n",
    "grad_h[h<0] = 0.\n",
    "grad_w1 = x.t().mm(grad_h)\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a learning step\n",
    "w1 -= learning_rate * grad_w1\n",
    "w2 += learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a 2nd order tensor, fill with zeroes\n",
    "torch.FloatTensor(3,3).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor from a list\n",
    "t = torch.Tensor([[2.],[1.]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze tensor, i.e turn a tensor with shape \n",
    "# of Ax1 into a tensor with shape A.\n",
    "ts = t.squeeze()\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product (special case of matrix multiplication)\n",
    "ts.dot(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose tensor\n",
    "t.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [1.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## concatenate tensors, but show only first 5 elements\n",
    "torch.cat([t]*5)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
